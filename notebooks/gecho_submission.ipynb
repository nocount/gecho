{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6410d846",
   "metadata": {},
   "source": [
    "# Gecho: Gemma Echo -- Automated Echocardiogram Reporting\n",
    "\n",
    "A RAG system that extracts key frames from echocardiogram videos, retrieves similar known cases via **MedSigLIP** embeddings, and generates clinical reports with **MedGemma**.\n",
    "\n",
    "**Models**: MedSigLIP-448 (retrieval) + MedGemma 1.5-4B-IT (generation)\n",
    "\n",
    "**Dataset**: [EchoNet-Dynamic](https://echonet.github.io/dynamic/) (Stanford)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d84e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup -- must run before any Keras imports\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "!pip install -q transformers keras keras-hub jax[cuda12] opencv-python faiss-cpu gradio Pillow huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b9ea4",
   "metadata": {},
   "source": [
    "## `config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7320f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configuration for Gecho pipeline.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "def _hf_login() -> None:\n",
    "    \"\"\"Authenticate with HuggingFace using Kaggle secrets or env var.\n",
    "\n",
    "    On Kaggle: stores HF token as a Kaggle Secret named \"HF_TOKEN\".\n",
    "    Locally: set the HF_TOKEN environment variable.\n",
    "    \"\"\"\n",
    "    token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "    # Try Kaggle Secrets API (available inside Kaggle notebooks)\n",
    "    if token is None:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if token:\n",
    "        try:\n",
    "            from huggingface_hub import login\n",
    "            login(token=token, add_to_git_credential=False)\n",
    "            print(\"Authenticated with HuggingFace.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] HF login failed: {e}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"[WARN] No HF_TOKEN found. Gated models will fail unless \"\n",
    "            \"attached locally via Kaggle 'Add Model'.\"\n",
    "        )\n",
    "\n",
    "def _find_local_model(model_name: str) -> str | None:\n",
    "    \"\"\"Search common Kaggle input paths for a locally-attached model.\n",
    "\n",
    "    Kaggle's \"Add Model\" feature places models under /kaggle/input/.\n",
    "    Users typically attach them with slugs like 'medsiglip-448' or\n",
    "    'medgemma-1-5-4b-it'.  We search for directories whose name\n",
    "    contains the key part of the model identifier.\n",
    "    \"\"\"\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return None\n",
    "\n",
    "    # Build search terms from the model name\n",
    "    # \"google/medsiglip-448\" -> \"medsiglip\"\n",
    "    # \"google/medgemma-1.5-4b-it\" -> \"medgemma\"\n",
    "    search_term = model_name.split(\"/\")[-1].split(\"-\")[0].lower()\n",
    "\n",
    "    for entry in sorted(kaggle_input.iterdir()):\n",
    "        if not entry.is_dir():\n",
    "            continue\n",
    "        if search_term in entry.name.lower():\n",
    "            # Kaggle model dirs can be nested: slug/framework/variant/version\n",
    "            # Walk down to find a directory with config.json or similar\n",
    "            for root, dirs, files in os.walk(entry):\n",
    "                if \"config.json\" in files or \"tokenizer.json\" in files:\n",
    "                    local_path = str(Path(root))\n",
    "                    print(f\"Found local model for '{model_name}': {local_path}\")\n",
    "                    return local_path\n",
    "            # If no config.json found, return the top-level match\n",
    "            local_path = str(entry)\n",
    "            print(f\"Found local model dir for '{model_name}': {local_path}\")\n",
    "            return local_path\n",
    "\n",
    "    return None\n",
    "\n",
    "@dataclass\n",
    "class GechoConfig:\n",
    "    \"\"\"Central configuration for the Gecho pipeline.\"\"\"\n",
    "\n",
    "    # --- Dataset paths (Kaggle defaults) ---\n",
    "    dataset_root: Path = Path(\"/kaggle/input/datasets/syxlicheng/heartdatabase/EchoNet-Dynamic\")\n",
    "    output_dir: Path = Path(\"/kaggle/working/gecho_output\")\n",
    "\n",
    "    # --- Model identifiers (HuggingFace Hub IDs) ---\n",
    "    medsiglip_model_id: str = \"google/medsiglip-448\"\n",
    "    medgemma_kerashub_preset: str = \"medgemma_1.5_instruct_4b\"\n",
    "    medgemma_hf_model_id: str = \"google/medgemma-1.5-4b-it\"\n",
    "\n",
    "    # --- Frame processing ---\n",
    "    siglip_frame_size: int = 448\n",
    "    medgemma_frame_size: int = 896\n",
    "\n",
    "    # --- FAISS / retrieval ---\n",
    "    faiss_top_k: int = 5\n",
    "    embedding_dim: int = 768  # verified at runtime from first embedding\n",
    "\n",
    "    # --- Generation parameters ---\n",
    "    max_new_tokens: int = 1024\n",
    "    sequence_length: int = 4096\n",
    "    dtype: str = \"bfloat16\"\n",
    "\n",
    "    # --- Derived paths (set in __post_init__) ---\n",
    "    videos_dir: Path = field(init=False)\n",
    "    file_list_path: Path = field(init=False)\n",
    "    volume_tracings_path: Path = field(init=False)\n",
    "    faiss_index_path: Path = field(init=False)\n",
    "    faiss_metadata_path: Path = field(init=False)\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        # Authenticate with HuggingFace for gated models\n",
    "        _hf_login()\n",
    "\n",
    "        self.videos_dir = self.dataset_root / \"Videos\"\n",
    "        self.file_list_path = self.dataset_root / \"FileList.csv\"\n",
    "        self.volume_tracings_path = self.dataset_root / \"VolumeTracings.csv\"\n",
    "        self.faiss_index_path = self.output_dir / \"gecho_faiss.index\"\n",
    "        self.faiss_metadata_path = self.output_dir / \"gecho_faiss_meta.pkl\"\n",
    "\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        # Resolve model paths: prefer locally-attached Kaggle models,\n",
    "        # fall back to HuggingFace Hub IDs (requires authentication).\n",
    "        self.medsiglip_model_id = (\n",
    "            _find_local_model(self.medsiglip_model_id)\n",
    "            or self.medsiglip_model_id\n",
    "        )\n",
    "        self.medgemma_hf_model_id = (\n",
    "            _find_local_model(self.medgemma_hf_model_id)\n",
    "            or self.medgemma_hf_model_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293f4c6",
   "metadata": {},
   "source": [
    "## `video_processor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Video processing pipeline for echocardiogram frame extraction.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Data classes\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class EchoFrames:\n",
    "    \"\"\"Extracted frames and metadata for a single echo video.\"\"\"\n",
    "    filename: str\n",
    "    ed_frame: np.ndarray        # RGB uint8, resized to siglip_frame_size\n",
    "    es_frame: np.ndarray        # RGB uint8, resized to siglip_frame_size\n",
    "    ed_frame_idx: int\n",
    "    es_frame_idx: int\n",
    "    ef: float | None = None     # Ejection fraction (ground truth)\n",
    "    esv: float | None = None    # End-systolic volume\n",
    "    edv: float | None = None    # End-diastolic volume\n",
    "    ef_category: str | None = None\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def classify_ef(ef: float) -> str:\n",
    "    \"\"\"Classify ejection fraction into clinical categories.\"\"\"\n",
    "    if ef >= 55:\n",
    "        return \"Normal\"\n",
    "    elif ef >= 45:\n",
    "        return \"Mild Dysfunction\"\n",
    "    elif ef >= 30:\n",
    "        return \"Moderate Dysfunction\"\n",
    "    else:\n",
    "        return \"Severe Dysfunction\"\n",
    "\n",
    "def load_file_list(config: GechoConfig, split: str = \"TRAIN\") -> pd.DataFrame:\n",
    "    \"\"\"Load EchoNet FileList.csv and filter by split.\n",
    "\n",
    "    Standard columns: FileName, EF, ESV, EDV, FrameHeight, FrameWidth,\n",
    "                      FPS, NumberOfFrames, Split\n",
    "    Some versions also include EDFrame, ESFrame directly.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(config.file_list_path)\n",
    "    df = df[df[\"Split\"].str.upper() == split.upper()].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def load_frame_indices(config: GechoConfig) -> dict[str, tuple[int, int]]:\n",
    "    \"\"\"Derive ED and ES frame indices from VolumeTracings.csv.\n",
    "\n",
    "    VolumeTracings.csv has one row per tracing point with columns:\n",
    "      FileName, X1, Y1, X2, Y2, Frame\n",
    "\n",
    "    Each video has tracings at exactly two frames (ED and ES).\n",
    "    The frame with the larger traced volume is ED (most dilated);\n",
    "    the smaller is ES (most contracted).\n",
    "\n",
    "    Returns dict mapping FileName -> (ed_frame_idx, es_frame_idx).\n",
    "    \"\"\"\n",
    "    if not config.volume_tracings_path.exists():\n",
    "        return {}\n",
    "\n",
    "    vt = pd.read_csv(config.volume_tracings_path)\n",
    "\n",
    "    # Each row is one tracing point.  Group by (FileName, Frame) to get\n",
    "    # a rough volume proxy: count of tracing points per frame, or use\n",
    "    # the traced coordinates to estimate area.  The simpler approach:\n",
    "    # the two unique frame numbers per video, the larger volume (more\n",
    "    # area enclosed) corresponds to ED.\n",
    "    frame_indices: dict[str, tuple[int, int]] = {}\n",
    "\n",
    "    for fname, group in vt.groupby(\"FileName\"):\n",
    "        frames = sorted(group[\"Frame\"].unique())\n",
    "        if len(frames) < 2:\n",
    "            # Fallback: only one traced frame\n",
    "            frame_indices[str(fname)] = (frames[0], frames[0])\n",
    "            continue\n",
    "\n",
    "        # Estimate enclosed area for each frame using the Shoelace formula\n",
    "        # on the tracing points (X1,Y1 -> X2,Y2 are inner/outer wall).\n",
    "        # Simpler proxy: sum of X2-X1 per frame ≈ cavity diameter sum.\n",
    "        areas: dict[int, float] = {}\n",
    "        for frame_num, fgroup in group.groupby(\"Frame\"):\n",
    "            # X1 = inner wall, X2 = outer wall (or vice versa).\n",
    "            # Cavity width at each tracing line ≈ |X1 - X2|.\n",
    "            areas[int(frame_num)] = float((fgroup[\"X2\"] - fgroup[\"X1\"]).abs().sum())\n",
    "\n",
    "        # ED = largest cavity (most dilated), ES = smallest (most contracted)\n",
    "        sorted_frames = sorted(areas.keys(), key=lambda f: areas[f], reverse=True)\n",
    "        ed_idx = sorted_frames[0]\n",
    "        es_idx = sorted_frames[-1]\n",
    "        frame_indices[str(fname)] = (ed_idx, es_idx)\n",
    "\n",
    "    return frame_indices\n",
    "\n",
    "def extract_frame(video_path: str | Path, frame_idx: int) -> np.ndarray:\n",
    "    \"\"\"Extract a single frame from a video file, returned as RGB uint8.\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    try:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            raise RuntimeError(\n",
    "                f\"Could not read frame {frame_idx} from {video_path}\"\n",
    "            )\n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "def resize_frame(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"Resize frame to (size, size) using bilinear interpolation.\"\"\"\n",
    "    return cv2.resize(frame, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Main extraction\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def extract_echo_frames(\n",
    "    video_path: str | Path,\n",
    "    row: pd.Series,\n",
    "    config: GechoConfig,\n",
    "    ed_idx: int | None = None,\n",
    "    es_idx: int | None = None,\n",
    ") -> EchoFrames:\n",
    "    \"\"\"Extract ED + ES frames for one video.\n",
    "\n",
    "    Frame indices can come from:\n",
    "      1. Explicit ed_idx/es_idx arguments (from VolumeTracings)\n",
    "      2. row[\"EDFrame\"] / row[\"ESFrame\"] columns (some CSV versions)\n",
    "      3. Heuristic fallback (frame 0 and frame at ~33%)\n",
    "    \"\"\"\n",
    "    # Resolve frame indices\n",
    "    if ed_idx is None:\n",
    "        if \"EDFrame\" in row.index:\n",
    "            ed_idx = int(row[\"EDFrame\"])\n",
    "        else:\n",
    "            ed_idx = 0\n",
    "\n",
    "    if es_idx is None:\n",
    "        if \"ESFrame\" in row.index:\n",
    "            es_idx = int(row[\"ESFrame\"])\n",
    "        else:\n",
    "            # Fallback: read total frame count and pick ~33%\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            cap.release()\n",
    "            es_idx = max(1, int(total * 0.33))\n",
    "\n",
    "    ed_raw = extract_frame(video_path, ed_idx)\n",
    "    es_raw = extract_frame(video_path, es_idx)\n",
    "\n",
    "    ed = resize_frame(ed_raw, config.siglip_frame_size)\n",
    "    es = resize_frame(es_raw, config.siglip_frame_size)\n",
    "\n",
    "    ef = float(row[\"EF\"]) if \"EF\" in row.index else None\n",
    "\n",
    "    return EchoFrames(\n",
    "        filename=row[\"FileName\"],\n",
    "        ed_frame=ed,\n",
    "        es_frame=es,\n",
    "        ed_frame_idx=ed_idx,\n",
    "        es_frame_idx=es_idx,\n",
    "        ef=ef,\n",
    "        esv=float(row[\"ESV\"]) if \"ESV\" in row.index and pd.notna(row.get(\"ESV\")) else None,\n",
    "        edv=float(row[\"EDV\"]) if \"EDV\" in row.index and pd.notna(row.get(\"EDV\")) else None,\n",
    "        ef_category=classify_ef(ef) if ef is not None else None,\n",
    "    )\n",
    "\n",
    "def extract_frames_from_upload(\n",
    "    video_path: str | Path,\n",
    "    config: GechoConfig,\n",
    ") -> EchoFrames:\n",
    "    \"\"\"Heuristic frame extraction for user-uploaded videos.\n",
    "\n",
    "    Without CSV metadata we use:\n",
    "      - ED = frame 0  (heart typically most dilated at start of clip)\n",
    "      - ES = frame at ~33% of total frames\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    try:\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total < 2:\n",
    "            raise RuntimeError(f\"Video too short ({total} frames): {video_path}\")\n",
    "\n",
    "        ed_idx = 0\n",
    "        es_idx = max(1, int(total * 0.33))\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, ed_idx)\n",
    "        ret, ed_raw = cap.read()\n",
    "        if not ret:\n",
    "            raise RuntimeError(f\"Cannot read ED frame from {video_path}\")\n",
    "        ed_raw = cv2.cvtColor(ed_raw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, es_idx)\n",
    "        ret, es_raw = cap.read()\n",
    "        if not ret:\n",
    "            raise RuntimeError(f\"Cannot read ES frame from {video_path}\")\n",
    "        es_raw = cv2.cvtColor(es_raw, cv2.COLOR_BGR2RGB)\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "    ed = resize_frame(ed_raw, config.siglip_frame_size)\n",
    "    es = resize_frame(es_raw, config.siglip_frame_size)\n",
    "\n",
    "    return EchoFrames(\n",
    "        filename=Path(video_path).name,\n",
    "        ed_frame=ed,\n",
    "        es_frame=es,\n",
    "        ed_frame_idx=ed_idx,\n",
    "        es_frame_idx=es_idx,\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Batch processing\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def process_dataset(\n",
    "    config: GechoConfig,\n",
    "    split: str = \"TRAIN\",\n",
    "    max_videos: int | None = None,\n",
    ") -> list[EchoFrames]:\n",
    "    \"\"\"Process the EchoNet dataset split, returning extracted frames.\"\"\"\n",
    "    df = load_file_list(config, split)\n",
    "    if max_videos is not None:\n",
    "        df = df.head(max_videos)\n",
    "\n",
    "    # Try to load ED/ES frame indices from VolumeTracings.csv\n",
    "    has_frame_cols = \"EDFrame\" in df.columns and \"ESFrame\" in df.columns\n",
    "    if has_frame_cols:\n",
    "        frame_map: dict[str, tuple[int, int]] = {}\n",
    "        print(\"Using EDFrame/ESFrame columns from FileList.csv\")\n",
    "    else:\n",
    "        print(\"EDFrame/ESFrame not in FileList.csv, loading VolumeTracings.csv ...\")\n",
    "        frame_map = load_frame_indices(config)\n",
    "        if frame_map:\n",
    "            print(f\"Loaded frame indices for {len(frame_map)} videos from VolumeTracings.csv\")\n",
    "        else:\n",
    "            print(\"[WARN] No VolumeTracings.csv found, using heuristic frame selection\")\n",
    "\n",
    "    results: list[EchoFrames] = []\n",
    "    for _, row in df.iterrows():\n",
    "        fname = row[\"FileName\"]\n",
    "        # EchoNet filenames may or may not have .avi extension\n",
    "        video_name = fname if fname.endswith(\".avi\") else f\"{fname}.avi\"\n",
    "        video_path = config.videos_dir / video_name\n",
    "\n",
    "        if not video_path.exists():\n",
    "            print(f\"[WARN] Video not found, skipping: {video_path}\")\n",
    "            continue\n",
    "\n",
    "        # Look up frame indices\n",
    "        ed_idx, es_idx = None, None\n",
    "        if not has_frame_cols and frame_map:\n",
    "            # VolumeTracings keys may or may not have .avi\n",
    "            indices = frame_map.get(fname) or frame_map.get(video_name)\n",
    "            if indices:\n",
    "                ed_idx, es_idx = indices\n",
    "\n",
    "        try:\n",
    "            frames = extract_echo_frames(\n",
    "                video_path, row, config, ed_idx=ed_idx, es_idx=es_idx\n",
    "            )\n",
    "            results.append(frames)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to process {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Processed {len(results)}/{len(df)} videos from {split} split.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd5a1a",
   "metadata": {},
   "source": [
    "## `embedding_engine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MedSigLIP embedding engine with FAISS retrieval.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Data classes\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class RetrievedCase:\n",
    "    \"\"\"A single retrieved case from the FAISS index.\"\"\"\n",
    "    filename: str\n",
    "    ef: float | None\n",
    "    ef_category: str | None\n",
    "    similarity_score: float\n",
    "    frame_type: str  # \"ED\" or \"ES\"\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Aggregated result from a FAISS query.\"\"\"\n",
    "    cases: list[RetrievedCase]\n",
    "    mean_ef: float | None = None\n",
    "    ef_std: float | None = None\n",
    "    consensus_category: str | None = None\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Index metadata\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class IndexEntry:\n",
    "    \"\"\"Metadata stored alongside each FAISS vector.\"\"\"\n",
    "    filename: str\n",
    "    frame_type: str  # \"ED\" or \"ES\"\n",
    "    ef: float | None = None\n",
    "    ef_category: str | None = None\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Embedding Engine\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class EmbeddingEngine:\n",
    "    \"\"\"MedSigLIP-based embedding engine with FAISS retrieval.\"\"\"\n",
    "\n",
    "    def __init__(self, config: GechoConfig) -> None:\n",
    "        self.config = config\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        print(f\"Loading MedSigLIP from {config.medsiglip_model_id} ...\")\n",
    "        self.processor = AutoProcessor.from_pretrained(config.medsiglip_model_id)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            config.medsiglip_model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "        ).to(self.device).eval()\n",
    "        print(f\"MedSigLIP loaded on {self.device}.\")\n",
    "\n",
    "        self.index: faiss.IndexFlatIP | None = None\n",
    "        self.metadata: list[IndexEntry] = []\n",
    "\n",
    "    # --- Encoding ---------------------------------------------------------\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_image(self, image: np.ndarray | Image.Image) -> np.ndarray:\n",
    "        \"\"\"Encode a single image to an L2-normalized embedding vector.\"\"\"\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        emb = self.model.get_image_features(**inputs)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        return emb.cpu().numpy().astype(np.float32).squeeze()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_batch(\n",
    "        self, images: list[np.ndarray | Image.Image], batch_size: int = 32\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Encode a batch of images to L2-normalized embeddings.\"\"\"\n",
    "        all_embs: list[np.ndarray] = []\n",
    "        pil_images = [\n",
    "            Image.fromarray(img) if isinstance(img, np.ndarray) else img\n",
    "            for img in images\n",
    "        ]\n",
    "\n",
    "        for i in range(0, len(pil_images), batch_size):\n",
    "            batch = pil_images[i : i + batch_size]\n",
    "            inputs = self.processor(images=batch, return_tensors=\"pt\").to(self.device)\n",
    "            emb = self.model.get_image_features(**inputs)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            all_embs.append(emb.cpu().numpy().astype(np.float32))\n",
    "\n",
    "        return np.concatenate(all_embs, axis=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Encode text to an L2-normalized embedding vector.\"\"\"\n",
    "        inputs = self.processor(text=[text], return_tensors=\"pt\", padding=True).to(\n",
    "            self.device\n",
    "        )\n",
    "        emb = self.model.get_text_features(**inputs)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        return emb.cpu().numpy().astype(np.float32).squeeze()\n",
    "\n",
    "    # --- Index building ---------------------------------------------------\n",
    "\n",
    "    def build_index(self, echo_frames_list: list[EchoFrames]) -> None:\n",
    "        \"\"\"Build a FAISS inner-product index from ED+ES frames.\"\"\"\n",
    "        images: list[np.ndarray] = []\n",
    "        entries: list[IndexEntry] = []\n",
    "\n",
    "        for ef in echo_frames_list:\n",
    "            for frame, ftype in [(ef.ed_frame, \"ED\"), (ef.es_frame, \"ES\")]:\n",
    "                images.append(frame)\n",
    "                entries.append(IndexEntry(\n",
    "                    filename=ef.filename,\n",
    "                    frame_type=ftype,\n",
    "                    ef=ef.ef,\n",
    "                    ef_category=ef.ef_category,\n",
    "                ))\n",
    "\n",
    "        print(f\"Encoding {len(images)} frames ...\")\n",
    "        embeddings = self.encode_batch(images)\n",
    "\n",
    "        # Update embedding_dim from actual data\n",
    "        dim = embeddings.shape[1]\n",
    "        if dim != self.config.embedding_dim:\n",
    "            print(f\"Updating embedding_dim: {self.config.embedding_dim} -> {dim}\")\n",
    "            self.config.embedding_dim = dim\n",
    "\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.index.add(embeddings)\n",
    "        self.metadata = entries\n",
    "        print(f\"FAISS index built: {self.index.ntotal} vectors, dim={dim}.\")\n",
    "\n",
    "    # --- Persistence ------------------------------------------------------\n",
    "\n",
    "    def save_index(self, config: GechoConfig | None = None) -> None:\n",
    "        \"\"\"Save FAISS index and metadata to disk.\"\"\"\n",
    "        cfg = config or self.config\n",
    "        if self.index is None:\n",
    "            raise RuntimeError(\"No index to save. Call build_index first.\")\n",
    "\n",
    "        faiss.write_index(self.index, str(cfg.faiss_index_path))\n",
    "        with open(cfg.faiss_metadata_path, \"wb\") as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "        print(f\"Index saved to {cfg.faiss_index_path}\")\n",
    "\n",
    "    def load_index(self, config: GechoConfig | None = None) -> None:\n",
    "        \"\"\"Load FAISS index and metadata from disk.\"\"\"\n",
    "        cfg = config or self.config\n",
    "        self.index = faiss.read_index(str(cfg.faiss_index_path))\n",
    "        with open(cfg.faiss_metadata_path, \"rb\") as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        print(f\"Index loaded: {self.index.ntotal} vectors.\")\n",
    "\n",
    "    # --- Querying ---------------------------------------------------------\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        image: np.ndarray | Image.Image,\n",
    "        frame_type: str = \"ED\",\n",
    "        top_k: int | None = None,\n",
    "    ) -> RetrievalResult:\n",
    "        \"\"\"Find the most similar cases in the FAISS index.\"\"\"\n",
    "        if self.index is None:\n",
    "            raise RuntimeError(\"No index loaded. Call build_index or load_index.\")\n",
    "\n",
    "        k = top_k or self.config.faiss_top_k\n",
    "        emb = self.encode_image(image).reshape(1, -1)\n",
    "        scores, indices = self.index.search(emb, k * 2)  # over-fetch to filter\n",
    "\n",
    "        cases: list[RetrievedCase] = []\n",
    "        seen: set[str] = set()\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < 0:\n",
    "                continue\n",
    "            entry = self.metadata[idx]\n",
    "            # Optionally filter by frame type; deduplicate by filename\n",
    "            if entry.filename in seen:\n",
    "                continue\n",
    "            seen.add(entry.filename)\n",
    "            cases.append(RetrievedCase(\n",
    "                filename=entry.filename,\n",
    "                ef=entry.ef,\n",
    "                ef_category=entry.ef_category,\n",
    "                similarity_score=float(score),\n",
    "                frame_type=entry.frame_type,\n",
    "            ))\n",
    "            if len(cases) >= (top_k or self.config.faiss_top_k):\n",
    "                break\n",
    "\n",
    "        # Aggregate statistics\n",
    "        efs = [c.ef for c in cases if c.ef is not None]\n",
    "        mean_ef = float(np.mean(efs)) if efs else None\n",
    "        ef_std = float(np.std(efs)) if efs else None\n",
    "\n",
    "        # Consensus category from most-common category\n",
    "        cats = [c.ef_category for c in cases if c.ef_category]\n",
    "        consensus = Counter(cats).most_common(1)[0][0] if cats else None\n",
    "\n",
    "        return RetrievalResult(\n",
    "            cases=cases,\n",
    "            mean_ef=mean_ef,\n",
    "            ef_std=ef_std,\n",
    "            consensus_category=consensus,\n",
    "        )\n",
    "\n",
    "    # --- Zero-shot classification -----------------------------------------\n",
    "\n",
    "    def zero_shot_classify(\n",
    "        self,\n",
    "        image: np.ndarray | Image.Image,\n",
    "        labels: list[str] | None = None,\n",
    "    ) -> dict[str, float]:\n",
    "        \"\"\"Zero-shot classification using image-text similarity.\n",
    "\n",
    "        Returns dict mapping label -> probability (sums to 1).\n",
    "        \"\"\"\n",
    "        if labels is None:\n",
    "            labels = [\n",
    "                \"echocardiogram showing normal cardiac function\",\n",
    "                \"echocardiogram showing mild left ventricular dysfunction\",\n",
    "                \"echocardiogram showing moderate left ventricular dysfunction\",\n",
    "                \"echocardiogram showing severe left ventricular dysfunction\",\n",
    "            ]\n",
    "\n",
    "        img_emb = self.encode_image(image)\n",
    "        text_embs = np.array([self.encode_text(lbl) for lbl in labels])\n",
    "\n",
    "        # Cosine similarities (already L2-normalized)\n",
    "        sims = text_embs @ img_emb\n",
    "        # Softmax\n",
    "        exp_sims = np.exp(sims - sims.max())\n",
    "        probs = exp_sims / exp_sims.sum()\n",
    "\n",
    "        # Use short labels for display\n",
    "        short_labels = [\"Normal\", \"Mild Dysfunction\", \"Moderate Dysfunction\", \"Severe Dysfunction\"]\n",
    "        if len(short_labels) == len(labels):\n",
    "            return dict(zip(short_labels, probs.tolist()))\n",
    "        return dict(zip(labels, probs.tolist()))\n",
    "\n",
    "    # --- Cleanup ----------------------------------------------------------\n",
    "\n",
    "    def unload(self) -> None:\n",
    "        \"\"\"Free GPU memory by deleting the model.\"\"\"\n",
    "        del self.model\n",
    "        del self.processor\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"MedSigLIP unloaded from GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba614949",
   "metadata": {},
   "source": [
    "## `report_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1456128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MedGemma report generation with KerasHub (primary) and transformers (fallback).\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Data classes\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class ClinicalReport:\n",
    "    \"\"\"Generated clinical report for an echocardiogram analysis.\"\"\"\n",
    "    summary: str\n",
    "    ef_assessment: str\n",
    "    retrieval_context: str\n",
    "    full_report: str\n",
    "    confidence_note: str\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Prompt builders\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert echocardiography AI assistant helping clinicians \"\n",
    "    \"interpret echocardiogram images. You provide structured, evidence-based \"\n",
    "    \"assessments. Always include a disclaimer that findings require \"\n",
    "    \"verification by a qualified cardiologist.\"\n",
    ")\n",
    "\n",
    "def _build_retrieval_context(result: RetrievalResult) -> str:\n",
    "    \"\"\"Format retrieval results into a context string for the prompt.\"\"\"\n",
    "    if not result.cases:\n",
    "        return \"No similar cases were found in the reference database.\"\n",
    "\n",
    "    lines = [\n",
    "        f\"Retrieved {len(result.cases)} similar cases from EchoNet-Dynamic:\"\n",
    "    ]\n",
    "    for i, c in enumerate(result.cases, 1):\n",
    "        ef_str = f\"EF={c.ef:.1f}%\" if c.ef is not None else \"EF=N/A\"\n",
    "        lines.append(\n",
    "            f\"  {i}. {c.filename} ({c.frame_type}) - {ef_str} \"\n",
    "            f\"({c.ef_category}), similarity={c.similarity_score:.3f}\"\n",
    "        )\n",
    "\n",
    "    if result.mean_ef is not None:\n",
    "        lines.append(\n",
    "            f\"Mean EF of similar cases: {result.mean_ef:.1f}% \"\n",
    "            f\"(SD: {result.ef_std:.1f}%)\"\n",
    "        )\n",
    "    if result.consensus_category:\n",
    "        lines.append(f\"Consensus category: {result.consensus_category}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _build_zeroshot_context(scores: dict[str, float]) -> str:\n",
    "    \"\"\"Format zero-shot scores into a context string.\"\"\"\n",
    "    if not scores:\n",
    "        return \"\"\n",
    "    lines = [\"MedSigLIP zero-shot classification:\"]\n",
    "    for label, prob in sorted(scores.items(), key=lambda x: -x[1]):\n",
    "        lines.append(f\"  - {label}: {prob:.1%}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _build_single_frame_prompt(\n",
    "    frame_type: str,\n",
    "    retrieval_ctx: str,\n",
    "    zeroshot_ctx: str,\n",
    ") -> str:\n",
    "    \"\"\"Build a prompt for single-frame analysis.\"\"\"\n",
    "    return (\n",
    "        f\"{SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"Analyze this {frame_type} (End-Diastole = ED, End-Systole = ES) \"\n",
    "        f\"echocardiogram frame.\\n\\n\"\n",
    "        f\"### Context from Similar Cases\\n{retrieval_ctx}\\n\\n\"\n",
    "        f\"### Zero-Shot Classification\\n{zeroshot_ctx}\\n\\n\"\n",
    "        f\"### Requested Output\\n\"\n",
    "        f\"Provide a structured report with the following sections:\\n\"\n",
    "        f\"1. **Visual Assessment**: Describe left ventricle size, wall motion, \"\n",
    "        f\"and any visible abnormalities.\\n\"\n",
    "        f\"2. **EF Estimate**: Based on the visual features and similar-case \"\n",
    "        f\"context, estimate the ejection fraction range.\\n\"\n",
    "        f\"3. **Clinical Impression**: Summarize the key findings and their \"\n",
    "        f\"clinical significance.\\n\"\n",
    "        f\"4. **Limitations**: Note any caveats about this automated analysis.\"\n",
    "    )\n",
    "\n",
    "def _build_comparison_prompt(\n",
    "    ed_retrieval_ctx: str,\n",
    "    es_retrieval_ctx: str,\n",
    ") -> str:\n",
    "    \"\"\"Build a prompt for ED vs ES comparison analysis.\"\"\"\n",
    "    return (\n",
    "        f\"{SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"You are given two echocardiogram frames from the same patient:\\n\"\n",
    "        f\"- **Image 1**: End-Diastole (ED) frame — the heart is maximally dilated.\\n\"\n",
    "        f\"- **Image 2**: End-Systole (ES) frame — the heart is maximally contracted.\\n\\n\"\n",
    "        f\"### ED Similar Cases\\n{ed_retrieval_ctx}\\n\\n\"\n",
    "        f\"### ES Similar Cases\\n{es_retrieval_ctx}\\n\\n\"\n",
    "        f\"### Requested Output\\n\"\n",
    "        f\"Provide a structured report with the following sections:\\n\"\n",
    "        f\"1. **Visual Assessment**: Compare LV size between ED and ES. \"\n",
    "        f\"Describe wall motion and contractility.\\n\"\n",
    "        f\"2. **EF Estimate**: Based on the visual change between ED and ES \"\n",
    "        f\"plus similar-case context, estimate the ejection fraction range.\\n\"\n",
    "        f\"3. **Clinical Impression**: Key findings and clinical significance.\\n\"\n",
    "        f\"4. **Limitations**: Caveats about this automated analysis.\"\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Report Generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ReportGenerator:\n",
    "    \"\"\"MedGemma-based clinical report generator.\"\"\"\n",
    "\n",
    "    def __init__(self, config: GechoConfig) -> None:\n",
    "        self.config = config\n",
    "        self._backend = self._load_model()\n",
    "\n",
    "    def _load_model(self) -> str:\n",
    "        \"\"\"Load MedGemma via KerasHub (preferred) or transformers fallback.\"\"\"\n",
    "        # Try KerasHub first\n",
    "        try:\n",
    "            return self._load_kerashub()\n",
    "        except Exception as e:\n",
    "            print(f\"[INFO] KerasHub loading failed ({e}), trying transformers...\")\n",
    "            return self._load_transformers()\n",
    "\n",
    "    def _load_kerashub(self) -> str:\n",
    "        \"\"\"Load via keras_hub.\"\"\"\n",
    "        os.environ.setdefault(\"KERAS_BACKEND\", \"jax\")\n",
    "        import keras_hub  # noqa: E402\n",
    "\n",
    "        print(f\"Loading MedGemma via KerasHub: {self.config.medgemma_kerashub_preset}\")\n",
    "        self.keras_model = keras_hub.models.Gemma3CausalLM.from_preset(\n",
    "            self.config.medgemma_kerashub_preset,\n",
    "            dtype=self.config.dtype,\n",
    "        )\n",
    "        print(\"MedGemma loaded via KerasHub (JAX).\")\n",
    "        return \"kerashub\"\n",
    "\n",
    "    def _load_transformers(self) -> str:\n",
    "        \"\"\"Load via HuggingFace transformers.\"\"\"\n",
    "        import torch\n",
    "        from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "\n",
    "        print(f\"Loading MedGemma via transformers: {self.config.medgemma_hf_model_id}\")\n",
    "\n",
    "        dtype_map = {\"bfloat16\": torch.bfloat16, \"float16\": torch.float16}\n",
    "        torch_dtype = dtype_map.get(self.config.dtype, torch.bfloat16)\n",
    "\n",
    "        self.hf_processor = AutoProcessor.from_pretrained(\n",
    "            self.config.medgemma_hf_model_id\n",
    "        )\n",
    "        self.hf_model = AutoModelForImageTextToText.from_pretrained(\n",
    "            self.config.medgemma_hf_model_id,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.hf_model.eval()\n",
    "        print(\"MedGemma loaded via transformers (PyTorch).\")\n",
    "        return \"transformers\"\n",
    "\n",
    "    # --- Generation backends ----------------------------------------------\n",
    "\n",
    "    def _generate_kerashub(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        images: list[np.ndarray],\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text using KerasHub backend.\"\"\"\n",
    "        # KerasHub Gemma3 expects images passed alongside the prompt\n",
    "        # The prompt should contain <start_of_image> tokens for each image\n",
    "        image_tokens = \"\\n\".join([\"<start_of_image>\"] * len(images))\n",
    "        full_prompt = f\"{image_tokens}\\n{prompt}\"\n",
    "\n",
    "        response = self.keras_model.generate(\n",
    "            {\n",
    "                \"prompts\": full_prompt,\n",
    "                \"images\": [\n",
    "                    img.astype(\"float32\") / 255.0 if img.dtype == np.uint8 else img\n",
    "                    for img in images\n",
    "                ],\n",
    "            },\n",
    "            max_length=self.config.sequence_length,\n",
    "        )\n",
    "        # KerasHub returns the full sequence; strip the prompt portion\n",
    "        if isinstance(response, str):\n",
    "            return response\n",
    "        return str(response)\n",
    "\n",
    "    def _generate_transformers(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        images: list[np.ndarray],\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text using transformers backend.\"\"\"\n",
    "        import torch\n",
    "\n",
    "        pil_images = [\n",
    "            Image.fromarray(img) if isinstance(img, np.ndarray) else img\n",
    "            for img in images\n",
    "        ]\n",
    "\n",
    "        # Build chat messages with images\n",
    "        content: list[dict] = []\n",
    "        for img in pil_images:\n",
    "            content.append({\"type\": \"image\", \"image\": img})\n",
    "        content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "        inputs = self.hf_processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.hf_model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.hf_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                do_sample=False,\n",
    "            )\n",
    "\n",
    "        # Decode only the new tokens\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        generated = output_ids[0][input_len:]\n",
    "        return self.hf_processor.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "    def _generate(self, prompt: str, images: list[np.ndarray]) -> str:\n",
    "        \"\"\"Route to the active backend.\"\"\"\n",
    "        if self._backend == \"kerashub\":\n",
    "            return self._generate_kerashub(prompt, images)\n",
    "        return self._generate_transformers(prompt, images)\n",
    "\n",
    "    # --- Public API -------------------------------------------------------\n",
    "\n",
    "    def generate_single_frame_report(\n",
    "        self,\n",
    "        frame: np.ndarray,\n",
    "        frame_type: str,\n",
    "        retrieval_result: RetrievalResult,\n",
    "        zeroshot_scores: dict[str, float] | None = None,\n",
    "    ) -> ClinicalReport:\n",
    "        \"\"\"Generate a clinical report for a single echo frame.\"\"\"\n",
    "        retrieval_ctx = _build_retrieval_context(retrieval_result)\n",
    "        zeroshot_ctx = _build_zeroshot_context(zeroshot_scores or {})\n",
    "        prompt = _build_single_frame_prompt(frame_type, retrieval_ctx, zeroshot_ctx)\n",
    "\n",
    "        raw = self._generate(prompt, [frame])\n",
    "        return self._parse_report(raw, retrieval_ctx)\n",
    "\n",
    "    def generate_comparison_report(\n",
    "        self,\n",
    "        ed_frame: np.ndarray,\n",
    "        es_frame: np.ndarray,\n",
    "        ed_retrieval: RetrievalResult,\n",
    "        es_retrieval: RetrievalResult,\n",
    "    ) -> ClinicalReport:\n",
    "        \"\"\"Generate a clinical report comparing ED and ES frames.\"\"\"\n",
    "        ed_ctx = _build_retrieval_context(ed_retrieval)\n",
    "        es_ctx = _build_retrieval_context(es_retrieval)\n",
    "        prompt = _build_comparison_prompt(ed_ctx, es_ctx)\n",
    "\n",
    "        raw = self._generate(prompt, [ed_frame, es_frame])\n",
    "        combined_ctx = f\"--- ED ---\\n{ed_ctx}\\n\\n--- ES ---\\n{es_ctx}\"\n",
    "        return self._parse_report(raw, combined_ctx)\n",
    "\n",
    "    # --- Parsing ----------------------------------------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_report(raw_text: str, retrieval_ctx: str) -> ClinicalReport:\n",
    "        \"\"\"Parse raw model output into a structured ClinicalReport.\"\"\"\n",
    "        # Try to extract sections; fallback to raw text\n",
    "        sections = {\n",
    "            \"Visual Assessment\": \"\",\n",
    "            \"EF Estimate\": \"\",\n",
    "            \"Clinical Impression\": \"\",\n",
    "            \"Limitations\": \"\",\n",
    "        }\n",
    "\n",
    "        current_section = None\n",
    "        lines: list[str] = []\n",
    "\n",
    "        for line in raw_text.split(\"\\n\"):\n",
    "            matched = False\n",
    "            for key in sections:\n",
    "                if key.lower() in line.lower():\n",
    "                    if current_section and lines:\n",
    "                        sections[current_section] = \"\\n\".join(lines).strip()\n",
    "                    current_section = key\n",
    "                    lines = []\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                lines.append(line)\n",
    "\n",
    "        if current_section and lines:\n",
    "            sections[current_section] = \"\\n\".join(lines).strip()\n",
    "\n",
    "        # Build summary from first non-empty section\n",
    "        summary = (\n",
    "            sections[\"Clinical Impression\"]\n",
    "            or sections[\"Visual Assessment\"]\n",
    "            or raw_text[:500]\n",
    "        )\n",
    "\n",
    "        return ClinicalReport(\n",
    "            summary=summary,\n",
    "            ef_assessment=sections[\"EF Estimate\"] or \"See full report.\",\n",
    "            retrieval_context=retrieval_ctx,\n",
    "            full_report=raw_text,\n",
    "            confidence_note=(\n",
    "                sections[\"Limitations\"]\n",
    "                or \"This is an AI-generated analysis and must be reviewed \"\n",
    "                \"by a qualified cardiologist before clinical use.\"\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88c74c",
   "metadata": {},
   "source": [
    "## `ui`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c84748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gradio dashboard for Gecho echocardiogram analysis.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class GechoDashboard:\n",
    "    \"\"\"Three-column Gradio dashboard for echocardiogram analysis.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GechoConfig,\n",
    "        embedding_engine: EmbeddingEngine,\n",
    "        report_generator: ReportGenerator,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.engine = embedding_engine\n",
    "        self.generator = report_generator\n",
    "\n",
    "    # --- Analysis pipeline ------------------------------------------------\n",
    "\n",
    "    def analyze_video(\n",
    "        self,\n",
    "        video_file,\n",
    "        mode: str,\n",
    "        progress: gr.Progress = gr.Progress(),\n",
    "    ) -> tuple:\n",
    "        \"\"\"Main pipeline: extract -> embed -> retrieve -> classify -> generate.\n",
    "\n",
    "        Returns tuple of outputs matching the Gradio component order.\n",
    "        \"\"\"\n",
    "        if not video_file:\n",
    "            raise gr.Error(\"Please upload a video file first.\")\n",
    "\n",
    "        # gr.File returns a filepath string\n",
    "        video_path = video_file if isinstance(video_file, str) else video_file.name\n",
    "\n",
    "        # Step 1: Extract frames\n",
    "        progress(0.1, desc=\"Extracting frames...\")\n",
    "        frames: EchoFrames = extract_frames_from_upload(video_path, self.config)\n",
    "        ed_img = Image.fromarray(frames.ed_frame)\n",
    "        es_img = Image.fromarray(frames.es_frame)\n",
    "\n",
    "        # Step 2: Zero-shot classification on the primary frame\n",
    "        progress(0.3, desc=\"Running zero-shot classification...\")\n",
    "        primary_frame = frames.ed_frame if \"ED\" in mode else frames.es_frame\n",
    "        zeroshot_scores = self.engine.zero_shot_classify(primary_frame)\n",
    "\n",
    "        # Step 3: FAISS retrieval\n",
    "        progress(0.5, desc=\"Retrieving similar cases...\")\n",
    "        if mode == \"Comparison (ED vs ES)\":\n",
    "            ed_result = self.engine.query(frames.ed_frame, frame_type=\"ED\")\n",
    "            es_result = self.engine.query(frames.es_frame, frame_type=\"ES\")\n",
    "        elif \"ES\" in mode:\n",
    "            es_result = self.engine.query(frames.es_frame, frame_type=\"ES\")\n",
    "            ed_result = None\n",
    "        else:\n",
    "            ed_result = self.engine.query(frames.ed_frame, frame_type=\"ED\")\n",
    "            es_result = None\n",
    "\n",
    "        active_result = ed_result or es_result\n",
    "\n",
    "        # Step 4: Build retrieval table\n",
    "        retrieval_table = self._format_retrieval_table(active_result)\n",
    "\n",
    "        # Step 5: Generate report\n",
    "        progress(0.7, desc=\"Generating clinical report...\")\n",
    "        if mode == \"Comparison (ED vs ES)\" and ed_result and es_result:\n",
    "            report = self.generator.generate_comparison_report(\n",
    "                frames.ed_frame, frames.es_frame, ed_result, es_result\n",
    "            )\n",
    "        else:\n",
    "            frame = frames.ed_frame if ed_result else frames.es_frame\n",
    "            ftype = \"ED\" if ed_result else \"ES\"\n",
    "            report = self.generator.generate_single_frame_report(\n",
    "                frame, ftype, active_result, zeroshot_scores\n",
    "            )\n",
    "\n",
    "        progress(1.0, desc=\"Done!\")\n",
    "\n",
    "        # Format outputs\n",
    "        report_md = self._format_report_markdown(report)\n",
    "        zeroshot_label = {k: float(v) for k, v in zeroshot_scores.items()}\n",
    "\n",
    "        return (\n",
    "            ed_img,                 # ED frame display\n",
    "            es_img,                 # ES frame display\n",
    "            zeroshot_label,         # gr.Label (zero-shot)\n",
    "            retrieval_table,        # gr.Dataframe\n",
    "            report_md,              # gr.Markdown (report)\n",
    "        )\n",
    "\n",
    "    # --- Formatting helpers -----------------------------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_retrieval_table(\n",
    "        result: RetrievalResult | None,\n",
    "    ) -> list[list[str]]:\n",
    "        \"\"\"Format retrieval results as a table for gr.Dataframe.\"\"\"\n",
    "        if result is None or not result.cases:\n",
    "            return [[\"No results\", \"\", \"\", \"\"]]\n",
    "        rows = []\n",
    "        for c in result.cases:\n",
    "            rows.append([\n",
    "                c.filename,\n",
    "                f\"{c.ef:.1f}%\" if c.ef is not None else \"N/A\",\n",
    "                c.ef_category or \"N/A\",\n",
    "                f\"{c.similarity_score:.3f}\",\n",
    "            ])\n",
    "        return rows\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_report_markdown(report: ClinicalReport) -> str:\n",
    "        \"\"\"Format a ClinicalReport as Markdown for display.\"\"\"\n",
    "        return (\n",
    "            f\"## Clinical Report\\n\\n\"\n",
    "            f\"{report.full_report}\\n\\n\"\n",
    "            f\"---\\n\\n\"\n",
    "            f\"### EF Assessment\\n{report.ef_assessment}\\n\\n\"\n",
    "            f\"### Retrieval Context\\n\"\n",
    "            f\"```\\n{report.retrieval_context}\\n```\\n\\n\"\n",
    "            f\"### Confidence Note\\n\"\n",
    "            f\"> {report.confidence_note}\"\n",
    "        )\n",
    "\n",
    "    # --- Gradio app -------------------------------------------------------\n",
    "\n",
    "    def build(self) -> gr.Blocks:\n",
    "        \"\"\"Build and return the Gradio Blocks app.\"\"\"\n",
    "        with gr.Blocks(\n",
    "            title=\"Gecho - Automated Echocardiogram Analysis\",\n",
    "            theme=gr.themes.Soft(),\n",
    "        ) as app:\n",
    "            gr.Markdown(\n",
    "                \"# Gecho: Gemma Echo\\n\"\n",
    "                \"Automated echocardiogram interpretation powered by \"\n",
    "                \"MedSigLIP retrieval and MedGemma report generation.\"\n",
    "            )\n",
    "\n",
    "            with gr.Row():\n",
    "                # --- Left column: Input ---\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Input\")\n",
    "                    video_input = gr.File(\n",
    "                        label=\"Upload Echo Video (.avi / .mp4)\",\n",
    "                        file_types=[\".avi\", \".mp4\", \".mov\", \".mkv\"],\n",
    "                    )\n",
    "                    mode_selector = gr.Radio(\n",
    "                        choices=[\n",
    "                            \"Single Frame (ED)\",\n",
    "                            \"Single Frame (ES)\",\n",
    "                            \"Comparison (ED vs ES)\",\n",
    "                        ],\n",
    "                        value=\"Comparison (ED vs ES)\",\n",
    "                        label=\"Analysis Mode\",\n",
    "                    )\n",
    "                    analyze_btn = gr.Button(\n",
    "                        \"Analyze\", variant=\"primary\", size=\"lg\"\n",
    "                    )\n",
    "\n",
    "                    gr.Markdown(\"### Extracted Frames\")\n",
    "                    ed_display = gr.Image(label=\"End-Diastole (ED)\", type=\"pil\")\n",
    "                    es_display = gr.Image(label=\"End-Systole (ES)\", type=\"pil\")\n",
    "\n",
    "                # --- Middle column: Retrieval ---\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### MedSigLIP Classification\")\n",
    "                    zeroshot_label = gr.Label(\n",
    "                        label=\"Zero-Shot Cardiac Function\",\n",
    "                        num_top_classes=4,\n",
    "                    )\n",
    "\n",
    "                    gr.Markdown(\"### Similar Cases (FAISS Retrieval)\")\n",
    "                    retrieval_table = gr.Dataframe(\n",
    "                        headers=[\"Filename\", \"EF\", \"Category\", \"Similarity\"],\n",
    "                        label=\"Top Retrieved Cases\",\n",
    "                        interactive=False,\n",
    "                    )\n",
    "\n",
    "                # --- Right column: Report ---\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Generated Report\")\n",
    "                    report_output = gr.Markdown(\n",
    "                        value=\"*Upload a video and click Analyze to begin.*\"\n",
    "                    )\n",
    "\n",
    "                    gr.Markdown(\"### Human-in-the-Loop\")\n",
    "                    with gr.Row():\n",
    "                        approve_btn = gr.Button(\"Approve Report\", variant=\"secondary\")\n",
    "                        edit_btn = gr.Button(\"Edit Report\", variant=\"secondary\")\n",
    "                    status_text = gr.Textbox(\n",
    "                        label=\"Status\",\n",
    "                        value=\"Awaiting analysis...\",\n",
    "                        interactive=False,\n",
    "                    )\n",
    "\n",
    "            # --- Event handlers ---\n",
    "            analyze_btn.click(\n",
    "                fn=self.analyze_video,\n",
    "                inputs=[video_input, mode_selector],\n",
    "                outputs=[\n",
    "                    ed_display,\n",
    "                    es_display,\n",
    "                    zeroshot_label,\n",
    "                    retrieval_table,\n",
    "                    report_output,\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            approve_btn.click(\n",
    "                fn=lambda: \"Report approved by clinician.\",\n",
    "                outputs=[status_text],\n",
    "            )\n",
    "            edit_btn.click(\n",
    "                fn=lambda: \"Report flagged for clinician editing.\",\n",
    "                outputs=[status_text],\n",
    "            )\n",
    "\n",
    "        return app\n",
    "\n",
    "def launch_dashboard(\n",
    "    config: GechoConfig,\n",
    "    embedding_engine: EmbeddingEngine,\n",
    "    report_generator: ReportGenerator,\n",
    "    share: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Convenience function to build and launch the dashboard.\"\"\"\n",
    "    dashboard = GechoDashboard(config, embedding_engine, report_generator)\n",
    "    app = dashboard.build()\n",
    "    app.launch(share=share)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424b761",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2663db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Main pipeline: build index, load models, launch UI ----\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize configuration (paths auto-adjust for Kaggle)\n",
    "config = GechoConfig()\n",
    "\n",
    "# Step 1: Load or build FAISS index\n",
    "engine = EmbeddingEngine(config)\n",
    "\n",
    "if config.faiss_index_path.exists() and config.faiss_metadata_path.exists():\n",
    "    print(\"Found existing FAISS index, loading...\")\n",
    "    engine.load_index()\n",
    "else:\n",
    "    print(\"No existing index found. Processing EchoNet-Dynamic training set...\")\n",
    "    echo_frames = process_dataset(config, split=\"TRAIN\")\n",
    "    print(\"Building embedding index...\")\n",
    "    engine.build_index(echo_frames)\n",
    "    engine.save_index()\n",
    "    del echo_frames\n",
    "\n",
    "# Step 2: Load report generator (MedGemma)\n",
    "print(\"Loading MedGemma report generator...\")\n",
    "generator = ReportGenerator(config)\n",
    "\n",
    "# Step 3: Launch Gradio dashboard\n",
    "print(\"Launching Gecho dashboard...\")\n",
    "launch_dashboard(config, engine, generator, share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f10430",
   "metadata": {},
   "source": [
    "## Technical Notes & Citations\n",
    "\n",
    "### Architecture\n",
    "- **Retrieval**: MedSigLIP-448 encodes echo frames into 768-dim embeddings. FAISS IndexFlatIP performs cosine-similarity search against ~15K training vectors.\n",
    "- **Generation**: MedGemma 1.5-4B-IT receives the query frame + RAG context (similar cases, zero-shot scores) and produces a structured clinical report.\n",
    "- **VRAM Strategy**: MedSigLIP (float16, ~1.6GB) builds the index first, then MedGemma (bfloat16, ~8GB) is loaded for generation. Total < 16GB P100.\n",
    "\n",
    "### Citations\n",
    "- Ouyang et al. *Video-based AI for beat-to-beat assessment of cardiac function.* Nature, 2020. (EchoNet-Dynamic)\n",
    "- Yang et al. *Advancing Multimodal Medical Capabilities of Gemini.* arXiv:2405.03162, 2024. (MedGemma)\n",
    "- Radford et al. *Learning Transferable Visual Models From Natural Language Supervision.* ICML, 2021. (SigLIP heritage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
